{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "---\n",
    "\n",
    "[PyTorch](https://pytorch.org/docs/stable/index.html) is a framework for building trainable (automatically differentiable) directed acyclic graphs in dynamic manner (in cotrast with e.g. Tensorflow which builds static dags).   \n",
    "\n",
    "PyTorch's main building block are tensors (and it's highlevel abstractions e.g. `torch.nn` layers) and operations upon those tensors. Using PyTorch we can define minimization problems, which can be solved using `torch` optimization modules.\n",
    "\n",
    "**Overvoew of PyTorch package**\n",
    " - `torch.nn`  Highl-level abstractions useful for designing neural network architectures including various neural network layer types, loss functions and containers for more complex models.\n",
    " - `torch.nn.functional`  Similar as torch.nn, not defined in class manner but functional.\n",
    " - `torch.nn.init` Set of methods used for initialization of torch Tensor.\n",
    " - `torch.optim` Module with various optimizers and learning rate schedulers for training of neural networks.\n",
    " - `torch.utils.data` Collection of classes for data manipulation.\n",
    " - `torch.autograd`  Reverse automatic differentiation system which enables automatical computation of the gradients using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy with Numpy\n",
    "We can use similar methods as in NumPy to initialze and manipulate with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros([3, 3], dtype=torch.long, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60470409, 0.29962302, 0.01461841],\n",
       "       [0.21101405, 0.96114071, 0.70313904],\n",
       "       [0.59613189, 0.80732797, 0.88849557]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9865, 0.5884, 0.3125],\n",
       "        [0.5655, 0.4635, 0.6931],\n",
       "        [0.3914, 0.5660, 0.1078]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_tensor = np.array([[1, 2] ,[3, 4]], dtype=np.float)\n",
    "numpy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(numpy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor + torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 9., 16.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor * torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 10.],\n",
       "        [15., 22.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor.mm(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3710, -1.6887],\n",
       "        [-1.2106, -0.2434]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.normal_(torch_tensor)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensor[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 1., 2.],\n",
       "        [3., 4., 3., 4.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch_tensor, torch_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(torch_tensor, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 3.],\n",
       "        [2., 4.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(torch_tensor, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tensor properties\n",
    "All those attributes are related to optimizations we can use over tensors.\n",
    "\n",
    " - `.requires_grad`  Indication that we want to compute gradinet for this tensor. Pytorch will start to track all operations on it.\n",
    " - `.grad` After calling `y.backward()`, we have in `x.grad` (in case it requires_grad) gradinet defined as $\\frac{dy}{dx}$.\n",
    " - `.grad_fn` Reference to function that has created the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float, requires_grad=True)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 9., 16.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_m = tt * tt\n",
    "tt_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.5000, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_m = tt_m.mean()\n",
    "tt_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MeanBackward1 at 0x7f82783cc5f8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_m.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_m.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute gradinet of `tt_m` variable with respect to all `torch.Tensor`s with `.require_grad=True`.\n",
    "To calculate the gradients, we need to run the `tt_m.backward()`.  \n",
    "This will calculate the gradient for `tt_m` with respect to `tt`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial tt\\_m}{\\partial tt_x} = \\frac{\\partial}{\\partial tt_x}\\left[\\frac{1}{n}\\sum_i^n tt_i^2\\right] = \\frac{2}{n}tt_{i=x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.5000, 2.0000]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_m.backward()\n",
    "tt.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way how to stop collecting gradinet information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((tt * tt).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Definition\n",
    "PyTorch enables definition of neural networks with several level of abstraction. Let's eplore them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = torch.tensor([[0.20, 0.15],\n",
    "                            [0.30, 0.20],\n",
    "                            [0.86, 0.99],\n",
    "                            [0.91, 0.88]])\n",
    "\n",
    "label_batch = torch.tensor([[1.],\n",
    "                            [1.],\n",
    "                            [-1.],\n",
    "                            [-1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level approach\n",
    "Using just `torch.Tensor` and `torch.autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.8436],\n",
       "         [0.6168]], requires_grad=True),\n",
       " tensor([[0.5524]], requires_grad=True))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.randn(2, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\"))\n",
    "w2 = torch.randn(1, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\"))\n",
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3778938055038452\n",
      "5000 0.8985980153083801\n",
      "10000 0.8597437143325806\n",
      "15000 0.8263626098632812\n",
      "20000 0.8027422428131104\n",
      "25000 0.7764884829521179\n",
      "30000 0.7401544451713562\n",
      "35000 0.6867016553878784\n",
      "40000 0.6095514893531799\n",
      "45000 0.5110574960708618\n",
      "50000 0.4074777364730835\n"
     ]
    }
   ],
   "source": [
    "# After each iteration, we adjust w1 and w2 parameters.\n",
    "for training_iteration in range(training_iterations):\n",
    "    # Here is actual forward pass through simple nn with 2 layers defines by w1 and w2.\n",
    "    prediction = input_batch.mm(w1)\n",
    "    prediction = torch.tanh(prediction)\n",
    "    prediction = prediction.mm(w2)\n",
    "    prediction = torch.tanh(prediction)\n",
    "    \n",
    "    # We can calculate err as mean square error, we need to get single scalar number for optimizer.\n",
    "    loss = (prediction - label_batch).pow(2).mean()\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    # Here we compute all the gradients of variables\n",
    "    loss.backward()\n",
    "    \n",
    "    # We don't want to collect gradient information for optimization steps.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # Clear gradients for next interation, we don't want to cummulate it.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1628],\n",
       "        [ 0.3971],\n",
       "        [-0.9179],\n",
       "        [-0.5615]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check predictions.\n",
    "prediction = input_batch.mm(w1)\n",
    "prediction = torch.tanh(prediction)\n",
    "prediction = prediction.mm(w2)\n",
    "prediction = torch.tanh(prediction)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'w1': w1, 'w2': w2}, './ckpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./ckpt.pth')\n",
    "w1.data = state_dict['w1']\n",
    "w2.data = state_dict['w2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container approach\n",
    "Integrating `torch.nn.Module` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # In case we use basic tensors, we need to label them as trainable parameters of this Module.\n",
    "        self.w1 = torch.nn.Parameter(torch.randn(2, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\")))\n",
    "        self.w2 = torch.nn.Parameter(torch.randn(1, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\")))\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        prediction = input_batch.mm(self.w1)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        prediction = prediction.mm(self.w2)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        return prediction\n",
    "\n",
    "simple_nn = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.0158],\n",
       "         [ 0.2822]], requires_grad=True), Parameter containing:\n",
       " tensor([[0.2751]], requires_grad=True)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(simple_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9155999422073364\n",
      "5000 0.8290987610816956\n",
      "10000 0.8114342093467712\n",
      "15000 0.7887741923332214\n",
      "20000 0.7575080394744873\n",
      "25000 0.7120348215103149\n",
      "30000 0.6451467871665955\n",
      "35000 0.5541778206825256\n",
      "40000 0.45018261671066284\n",
      "45000 0.3520523011684418\n",
      "50000 0.27135515213012695\n"
     ]
    }
   ],
   "source": [
    "for training_iteration in range(training_iterations):\n",
    "    prediction = simple_nn(input_batch)\n",
    "    \n",
    "    loss = (prediction - label_batch).pow(2).mean()\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    simple_nn.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for p in simple_nn.parameters():\n",
    "            p -= p.grad * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2693],\n",
       "        [ 0.5811],\n",
       "        [-0.9699],\n",
       "        [-0.6380]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container approach with torch.nn and  torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import Linear, MSELoss, Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer_1 = Linear(2, 1)\n",
    "        self.layer_2 = Linear(1, 1)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        prediction = self.layer_1(input_batch)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        prediction = self.layer_2(prediction)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        return prediction\n",
    "\n",
    "simple_nn = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.5188, -0.4814]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.3630], requires_grad=True), Parameter containing:\n",
       " tensor([[-0.2864]], requires_grad=True), Parameter containing:\n",
       " tensor([0.6903], requires_grad=True)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(simple_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fce = MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = SGD(simple_nn.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.203975677490234\n",
      "5000 0.0017448768485337496\n",
      "10000 0.0008221292519010603\n",
      "15000 0.0005331849679350853\n",
      "20000 0.0003930902748834342\n",
      "25000 0.0003106834483332932\n",
      "30000 0.00025651470059528947\n",
      "35000 0.00021826289594173431\n",
      "40000 0.00018983696645591408\n",
      "45000 0.00016788342327345163\n",
      "50000 0.00015043055464047939\n"
     ]
    }
   ],
   "source": [
    "for training_iteration in range(training_iterations):\n",
    "    prediction = simple_nn(input_batch)\n",
    "    \n",
    "    loss = loss_fce(prediction, label_batch)\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9965],\n",
       "        [ 0.9927],\n",
       "        [-0.9948],\n",
       "        [-0.9934]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.load_state_dict(simple_nn.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container approach with torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn_seq = torch.nn.Sequential(\n",
    "    Linear(2, 1),\n",
    "    Tanh(),\n",
    "    Linear(1, 1),\n",
    "    Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fce = MSELoss(reduction='sum')\n",
    "optimizer = SGD(simple_nn_seq.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.6270904541015625\n",
      "5000 0.0017423416720703244\n",
      "10000 0.0008255556458607316\n",
      "15000 0.0005363304517231882\n",
      "20000 0.0003957200678996742\n",
      "25000 0.0003129152173642069\n",
      "30000 0.00025844344054348767\n",
      "35000 0.00021993886912241578\n",
      "40000 0.0001912871957756579\n",
      "45000 0.00016916720778681338\n",
      "50000 0.00015160514158196747\n"
     ]
    }
   ],
   "source": [
    "for training_iteration in range(training_iterations):\n",
    "    prediction = simple_nn_seq(input_batch)\n",
    "    \n",
    "    loss = loss_fce(prediction, label_batch)\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9963],\n",
       "        [ 0.9927],\n",
       "        [-0.9948],\n",
       "        [-0.9934]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn_seq(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "custom_relu = CustomReLU().apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_relu(torch.tensor([-1,0,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
