{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "---\n",
    "\n",
    "[PyTorch](https://pytorch.org/docs/stable/index.html) is a framework for building trainable (automatically differentiable) directed acyclic graphs in dynamic manner (in cotrast with e.g. Tensorflow which builds static dags).   \n",
    "\n",
    "PyTorch's main building block are tensors (and it's highlevel abstractions e.g. `torch.nn` layers) and operations upon those tensors. Using PyTorch we can define minimization problems, which can be solved using `torch` optimization modules.\n",
    "\n",
    "**Overvoew of PyTorch package**\n",
    " - `torch.nn`  Highl-level abstractions useful for designing neural network architectures including various neural network layer types, loss functions and containers for more complex models.\n",
    " - `torch.nn.functional`  Similar as torch.nn, not defined in class manner but functional.\n",
    " - `torch.nn.init` Set of methods used for initialization of torch Tensor.\n",
    " - `torch.optim` Module with various optimizers and learning rate schedulers for training of neural networks.\n",
    " - `torch.utils.data` Collection of classes for data manipulation.\n",
    " - `torch.autograd`  Reverse automatic differentiation system which enables automatical computation of the gradients using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy with Numpy\n",
    "We can use similar methods as in NumPy to initialze and manipulate with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros([3, 3], dtype=torch.long, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_tensor = np.array([[1, 2] ,[3, 4]], dtype=np.float)\n",
    "numpy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(numpy_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations with tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor + torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor * torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor.mm(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.init.normal_(torch_tensor)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float)\n",
    "torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([torch_tensor, torch_tensor], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unsqueeze(torch_tensor, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.transpose(torch_tensor, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tensor properties\n",
    "All those attributes are related to optimizations we can use over tensors.\n",
    "\n",
    " - `.requires_grad`  Indication that we want to compute gradinet for this tensor. Pytorch will start to track all operations on it.\n",
    " - `.grad` After calling `y.backward()`, we have in `x.grad` (in case it requires_grad) gradinet defined as $\\frac{dy}{dx}$.\n",
    " - `.grad_fn` Reference to function that has created the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = torch.tensor([[1, 2] ,[3, 4]], dtype=torch.float, requires_grad=True)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_m = tt * tt\n",
    "tt_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_m = tt_m.mean()\n",
    "tt_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_m.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_m.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute gradinet of `tt_m` variable with respect to all `torch.Tensor`s with `.require_grad=True`.\n",
    "To calculate the gradients, we need to run the `tt_m.backward()`.  \n",
    "This will calculate the gradient for `tt_m` with respect to `tt`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial tt\\_m}{\\partial tt_x} = \\frac{\\partial}{\\partial tt_x}\\left[\\frac{1}{n}\\sum_i^n tt_i^2\\right] = \\frac{2}{n}tt_{i=x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_m.backward()\n",
    "tt.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way how to stop collecting gradinet information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print((tt * tt).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Definition\n",
    "PyTorch enables definition of neural networks with several level of abstraction. Let's eplore them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = torch.tensor([[0.20, 0.15],\n",
    "                            [0.30, 0.20],\n",
    "                            [0.86, 0.99],\n",
    "                            [0.91, 0.88]])\n",
    "\n",
    "label_batch = torch.tensor([[1.],\n",
    "                            [1.],\n",
    "                            [-1.],\n",
    "                            [-1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low level approach\n",
    "Using just `torch.Tensor` and `torch.autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainable parameters.\n",
    "w1 = torch.randn(2, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\"))\n",
    "w2 = torch.randn(1, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\"))\n",
    "w1, w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After each iteration, we adjust w1 and w2 parameters.\n",
    "for training_iteration in range(training_iterations):\n",
    "    # Here is actual forward pass through simple nn with 2 layers defines by w1 and w2.\n",
    "    prediction = input_batch.mm(w1)\n",
    "    prediction = torch.tanh(prediction)\n",
    "    prediction = prediction.mm(w2)\n",
    "    prediction = torch.tanh(prediction)\n",
    "    \n",
    "    # We can calculate err as mean square error, we need to get single scalar number for optimizer.\n",
    "    loss = (prediction - label_batch).pow(2).mean()\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    # Here we compute all the gradients of variables\n",
    "    loss.backward()\n",
    "    \n",
    "    # We don't want to collect gradient information for optimization steps.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        # Clear gradients for next interation, we don't want to cummulate it.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check predictions.\n",
    "prediction = input_batch.mm(w1)\n",
    "prediction = torch.tanh(prediction)\n",
    "prediction = prediction.mm(w2)\n",
    "prediction = torch.tanh(prediction)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'w1': w1, 'w2': w2}, './models/ckpt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./models/ckpt.pth')\n",
    "w1.data = state_dict['w1']\n",
    "w2.data = state_dict['w2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container approach\n",
    "Integrating `torch.nn.Module` container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # In case we use basic tensors, we need to label them as trainable parameters of this Module.\n",
    "        self.w1 = torch.nn.Parameter(torch.randn(2, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\")))\n",
    "        self.w2 = torch.nn.Parameter(torch.randn(1, 1, dtype=torch.float, requires_grad=True, device=torch.device(\"cpu\")))\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        prediction = input_batch.mm(self.w1)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        prediction = prediction.mm(self.w2)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        return prediction\n",
    "\n",
    "simple_nn = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(simple_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_iteration in range(training_iterations):\n",
    "    prediction = simple_nn(input_batch)\n",
    "    \n",
    "    loss = (prediction - label_batch).pow(2).mean()\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    simple_nn.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for p in simple_nn.parameters():\n",
    "            p -= p.grad * learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container approach with torch.nn and  torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import Linear, MSELoss, Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer_1 = Linear(2, 1)\n",
    "        self.layer_2 = Linear(1, 1)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        prediction = self.layer_1(input_batch)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        prediction = self.layer_2(prediction)\n",
    "        prediction = torch.tanh(prediction)\n",
    "        return prediction\n",
    "\n",
    "simple_nn = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(simple_nn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fce = MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(simple_nn.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_iteration in range(training_iterations):\n",
    "    prediction = simple_nn(input_batch)\n",
    "    \n",
    "    loss = loss_fce(prediction, label_batch)\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.load_state_dict(simple_nn.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Container approach with torch.nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "training_iterations = 55000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn_seq = torch.nn.Sequential(\n",
    "    Linear(2, 1),\n",
    "    Tanh(),\n",
    "    Linear(1, 1),\n",
    "    Tanh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fce = MSELoss(reduction='sum')\n",
    "optimizer = SGD(simple_nn_seq.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_iteration in range(training_iterations):\n",
    "    prediction = simple_nn_seq(input_batch)\n",
    "    \n",
    "    loss = loss_fce(prediction, label_batch)\n",
    "    if training_iteration % 5000 == 0:\n",
    "        print(training_iteration, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn_seq(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "custom_relu = CustomReLU().apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_relu(torch.tensor([-1,0,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
